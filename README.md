# NumericalOptimizationAlgorithms

In this notebook I implemented:
  - the Adam (Batch Version) algorithm to train a multivariable linear regression model.
  - the Adam (mini-batch) algorithm to train a multivariable linear regression model.

The implementation supported by the following curves.
    - Loss vs. iterations.
    - Loss vs. each parameter (i.e. loss vs theta 0, loss vs. theta 1 .... etc.).
    
    
